{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0220,  0.0503,  0.0336,  0.0539,  0.0822, -0.0558, -0.0212, -0.0848,\n",
      "         -0.0056, -0.0509]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2912, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000018C003E14E0>\n",
      "<AddmmBackward object at 0x0000018C003E1550>\n",
      "<AccumulateGrad object at 0x0000018C003E14E0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0018,  0.0045,  0.0271,  0.0151, -0.0141, -0.0060])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math \n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # seeding for random number generation\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        #training the model to make accurate predictions while adjusting weights continually\n",
    "        for iteration in range(training_iterations):\n",
    "            #siphon the training data via  the neuron\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            #computing error rate for back-propagation\n",
    "            error = training_outputs - output\n",
    "            \n",
    "            #performing weight adjustments\n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
    "\n",
    "            self.synaptic_weights += adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Randomly Generated Weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #initializing the neuron class\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Beginning Randomly Generated Weights: \")\n",
    "    print(neural_network.synaptic_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib, numpy and math \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math \n",
    "  \n",
    "x = np.linspace(-10, 10, 100) \n",
    "z = 1/(1 + np.exp(-x)) \n",
    "  \n",
    "plt.plot(x, z) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\") \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAa10lEQVR4nO3df5RcZZ3n8fcn3en8gECAhID5QYIGhiyjwrSsB4VVgZnAccgy6hj2zOo4jNndI7PO6M4RD7ssy+4cR11n5jiLw2RGFuQMMKijEz1hcFUYnXGiCfIzREITUdpgEqETAv2jqru/+0fdgqKp6qruqtvVT9fndU6drnrqVtU3t27n08/z3B+KCMzMrHPNa3cBZmbWXg4CM7MO5yAwM+twDgIzsw7nIDAz63Dd7S5gqpYtWxZr165tdxlmZkm5//77fxERy6s9l1wQrF27ll27drW7DDOzpEj6Sa3nPDRkZtbhHARmZh3OQWBm1uEcBGZmHc5BYGbW4XILAkk3Szoo6dEaz0vSZyX1SXpY0rl51WJmZrXl2SO4Bdg4yfOXAuuz2xbgL3KsxczMasjtOIKI+I6ktZMssgn4QpTOg71D0lJJp0bEM3nVZNZuI6NjPNJ/hP6BIZ4fLlIcC4pj44yOjVMcC2bspPA+/XySLjprBW9YvbTl79vOA8pWAk9XPO7P2l4VBJK2UOo1sGbNmhkpzqyVCqPjfOLuPXxpVz9HR0bbXQ4AUrsrsKk6+biFcy4Iqm2GVf9MiYitwFaA3t5e/yljSXlhZJT/cNsu/rnvWa44ZyUbzz6F1y4/lqWL5zO/ax7zu8T8rnl0zxPy/87WBu0Mgn5gdcXjVcD+NtVilpub7nuS7z35LJ95zxt416+sanc5Zq/Szt1HtwHvy/YeejNwxPMDNtcMF8e4/Qc/5eKzVjgEbNbKrUcg6Q7gbcAySf3AfwfmA0TETcB24DKgDxgEPpBXLWbt8rWH9vPciwV++/y17S7FrKY89xq6ss7zAXwor883mw1u2/ET1p98LOe/9qR2l2JWk48sNsvJUGGMR352hMt++VRPAtus5iAwy8mTh14gAs48ZUm7SzGblIPALCd7DxwF4IwVx7a5ErPJOQjMcrL3wAvM7xKnnXRMu0sxm5SDwCwnTxw4yunLjmV+l3/NbHbzFmqWk70Hj7Lew0KWAAeBWQ4GC6M8/dwQZ6zwRLHNfg4Csxz0HXwB8ESxpcFBYJaDvQdKQbDePQJLgIPALAf7Dw8BsPqExW2uxKw+B4FZDgYGCxy7oJuebv+K2eznrdQsB0cGiyxdPL/dZZg1xEFgloOBwYKDwJLhIDDLweGhIics7ml3GWYNcRCY5eDwYJGlDgJLhIPALAcDgwWWLvLQkKXBQWDWYuPjwZGhIid4jsAS4SAwa7Hnh4tEwPEeGrJEOAjMWmxgsAjgHoElw0Fg1mKHBwsA3mvIkuEgMGuxw1mP4Hj3CCwRDgKzFhtwj8AS4yAwa7Fyj8C7j1oqHARmLXZ4sIAExzkILBEOArMWGxgscvyi+XTNU7tLMWuIg8CsxQ4PFT0sZElxEJi12OHBgs8zZElxEJi12GFfi8AS4yAwa7GBwYJ3HbWkOAjMWsw9AkuNg8CshcbHgxdGRlmyoLvdpZg1LNcgkLRR0uOS+iRdU+X5NZLulfSApIclXZZnPWZ5GxkdB2BRj4PA0pFbEEjqAm4ELgU2AFdK2jBhsf8K3BUR5wCbgc/lVY/ZTBgqjgGwuKerzZWYNS7PHsF5QF9E7IuIAnAnsGnCMgEcl90/HtifYz1muRssjAKwaL6DwNKRZxCsBJ6ueNyftVW6HvgtSf3AduD3qr2RpC2SdknadejQoTxqNWuJ4axHsNA9AktInkFQ7fj6mPD4SuCWiFgFXAbcJulVNUXE1ojojYje5cuX51CqWWsMFbI5AvcILCF5BkE/sLri8SpePfRzFXAXQET8C7AQWJZjTWa5Ks8ROAgsJXkGwU5gvaR1knooTQZvm7DMT4GLACSdRSkIPPZjyXopCHq8Z7alI7etNSJGgauBe4A9lPYO2i3pBkmXZ4t9FPigpIeAO4DfjoiJw0dmyRgqZHME7hFYQnLd2TkitlOaBK5su67i/mPAW/KswWwmDXtoyBLk/qtZCw0WyscR+IAyS4eDwKyFPFlsKXIQmLXQy8cR+FfL0uGt1ayFhgpjzBP0dPlXy9LhrdWshYaKYyya34Xk6xVbOhwEZi00VBxjkU8vYYlxEJi10HBhzMcQWHIcBGYtNFQc8ymoLTkOArMWGiyMeddRS46DwKyFhooeGrL0OAjMWmjYk8WWIAeBWQsNeWjIEuQgMGuh8nEEZilxEJi10HBxzJeptOQ4CMxaaKgwxmL3CCwxDgKzFokIBj1ZbAlyEJi1yMjoOBG+Opmlx0Fg1iK+OpmlykFg1iIvX7jeQWBpcRCYtUj5wvXuEVhqHARmLVLuEXiOwFLjIDBrkfIcgc8+aqlxEJi1yFBhHPAcgaXHQWDWIoOFUcBzBJYeB4FZi3iOwFLlIDBrkWHvPmqJchCYtYh3H7VUOQjMWmSomE0WOwgsMd2NLijpBOA1wBDwVESM51aVWYJeniPw31eWlkmDQNLxwIeAK4Ee4BCwEFghaQfwuYi4N/cqzRIwUhxjQfc8JLW7FLMpqfeny5eAp4ELIuLMiHhrRPRGxGrgj4FNkq6q9WJJGyU9LqlP0jU1lvlNSY9J2i3p9mn/S8zabGR0nAXd7g1YeibtEUTEJZM8dz9wf63nJXUBNwKXAP3ATknbIuKximXWAx8H3hIRA5JOnmL9ZrPGyOgYCzw/YAma9M8XSe+q0d4j6b/Vee/zgL6I2BcRBeBOYNOEZT4I3BgRAwARcbCxss1mn5GiewSWpnpb7RZJd0taV26QdCnwMHBSndeupDSsVNaftVU6AzhD0j9L2iFpY7U3krRF0i5Juw4dOlTnY83aw0NDlqp6Q0O/JulK4JvZ+P3ZwHLgvRHxUJ33rjZjFlU+fz3wNmAV8F1JZ0fE4Ql1bAW2AvT29k58D7NZYWR0jAXdHhqy9DSy++hdwL8C/gA4DLwjIvY28Lp+YHXF41XA/irL7IiIIvBjSY9TCoadDby/2awyMjrOAu86agmqN0fwVuABSsNAq4Grga9JukHSgjrvvRNYL2mdpB5gM7BtwjJfBd6efdYySkNF+6b8rzCbBUaK4yx0j8ASVO/Plz8Dfjci/lNEDETEV4FzgAXApENDETFKKTjuAfYAd0XE7ixELs8Wuwd4VtJjwL3AH0bEs038e8zaprTXkHsElp56Q0PnTTyCOCIGgY9JuqXem0fEdmD7hLbrKu4H8JHsZpY0TxZbqupttefXeiIi9kg6TtLZLa7JLEmlIPDQkKWnXo/gXZI+BfwDpYPHyqeYeB2lsf3TgI/mWqFZIsqnmDBLTb3dR/8gO9ncu4H3AKdSOuncHuAvI+Kf8i/RLA3ea8hSVXf30eyo37/KbmZWg4eGLFX1zj466SRuRPxJa8sxS1fpgDL3CCw99XoES7KfZwJv4uXjAH4d+E5eRZmlZmw8KI6FewSWpHpzBP8DQNI3gHMj4mj2+Hrgi7lXZ5aIwmhpL2vPEViKGt1q1wCFiscFYG3LqzFL1Mho6epkHhqyFDV6qcrbgB9I+gqlE8ddAXwht6rMEjNS7hF4aMgS1FAQRMQfSbobuCBr+kBEPJBfWWZpGSmWg8A9AktPvb2GjouI5yWdCDyV3crPnRgRz+VbnlkaXhoa8hyBJahej+B24J2UjioOXnmNgQBOz6kus6QMFz00ZOmqt9fQO7Of6yZbzqzTebLYUtboZDHZqaMvzB7eFxFfz6cks/S8PFnsILD0NLTVSvpj4MPAY9ntw5I+kWdhZil5eY7AQ0OWnkZ7BJcBbyxfm0DSrZSuXPbxvAozS4n3GrKUTWWrXVpx//hWF2KWMg8NWcoa7RF8AnhA0r2U9hy6EPcGzF7ioSFLWaMHlN0h6T5KJ54T8LGI+HmehZmlxD0CS9lUttrl2c8u4HxJv5FDPWZJ8hyBpayhHoGkm4HXA7uB8sXsA/i7nOoyS8rLxxF4aMjS0+gcwZsjYkOulZglbGR0HAnmd6n+wmazTKP92H+R5CAwq6F0mcp5SA4CS0+jPYJbKYXBz4ERShPGERGvz60ys4SMFMc8LGTJajQIbgb+PfAIL88RmFlmZHSchT7zqCWq0SD4aURsq7+YWWcqDQ25R2BpajQIfiTpduBrlIaGAIgI7zVkRmmvIe86aqlqNAgWUQqAX61o8+6jZpmR4rgvSmPJavTI4g/kXYhZyjw0ZClr9ICyz1ZpPgLsioi/b21JZunx0JClrNEtdyHwRuCJ7PZ64ETgKkl/llNtZskoH0dglqJGt9zXAe+IiD+PiD8HLgbOAq7glfMGryBpo6THJfVJumaS5d4tKST1TqV4s9lipOihIUtXo0GwEjim4vExwGsiYoyKvYgqSeoCbgQuBTYAV1Y7OlnSEuA/A9+fQt1ms8rI6Jgniy1ZjW65nwIelPR/Jd1C6epk/1vSMcA3a7zmPKAvIvZFRAG4E9hUZbn/mb3/8JQqN5tFPDRkKWtoy42IzwPnA1/Nbm+NiL+OiBcj4g9rvGwl8HTF4/6s7SWSzgFWR8TXJ/t8SVsk7ZK069ChQ42UbDajvNeQpWzSIJD0S9nPc4FTKf3H/lPglKxt0pdXaYuK954H/Cnw0XpFRsTWiOiNiN7ly5fXW9xsxpXONeQegaWp3u6jHwG2AJ/JHseE598xyWv7gdUVj1cB+yseLwHOBu7Lzth4CrBN0uURsatOXWazysioDyizdNXbcv9a0ikR8faIeDuls5C+ADwKvLvOa3cC6yWtk9QDbAZeOl9RRByJiGURsTYi1gI7AIeAJWd0bJzR8fDQkCWrXhDcBBQAJF1I6SL2t1I6mGzrZC+MiFHgauAeYA9wV0TslnSDpMubLdxstvD1ii119YaGuiLiuez+e4GtEfFl4MuSHqz35hGxHdg+oe26Gsu+rX65ZrPPULF0mcpFPe4RWJrq/QnTJakcFhcB3654rtET1pnNaUOFUhAsnO8gsDTV+8/8DuAfJf0CGAK+CyDpdZSGh8w63nDWI1jsHoElatIgiIg/kvQtSruOfiMiynsNzQN+L+/izFLw0tCQewSWqLrDOxGxo0rb3nzKMUtPeWjIQWCp8m4OZk0azHoECz00ZIlyEJg1adg9Akucg8CsSZ4jsNQ5CMya5OMILHUOArMm+TgCS52DwKxJPo7AUucgMGvSUHGM7nlifpd/nSxN3nLNmjRUGPdEsSXNQWDWpKHimI8hsKQ5CMyaNFQYdY/AkuYgMGvSUHHMQWBJcxCYNWmoOO6hIUuag8CsScOFMRa7R2AJcxCYNWmoOOajii1pDgKzJnmOwFLnIDBr0lBhzKeXsKQ5CMyaNFwcY1GPf5UsXd56zZrkoSFLnYPArAkR4SCw5DkIzJowMjpOhC9TaWlzEJg1oXwtAh9HYClzEJg1wVcns7nAQWDWhHIQePdRS5mDwKwJ5aEhTxZbyhwEZk0Y9tCQzQEOArMmvDRH4B6BJSzXIJC0UdLjkvokXVPl+Y9IekzSw5K+Jem0POsxa7Xy0JDnCCxluQWBpC7gRuBSYANwpaQNExZ7AOiNiNcDXwI+lVc9ZnnwXkM2F+TZIzgP6IuIfRFRAO4ENlUuEBH3RsRg9nAHsCrHesxa7qXjCBwElrA8g2Al8HTF4/6srZargLurPSFpi6RdknYdOnSohSWaNcdzBDYX5BkEqtIWVReUfgvoBT5d7fmI2BoRvRHRu3z58haWaNYcH0dgc0F3ju/dD6yueLwK2D9xIUkXA9cC/yYiRnKsx6zlhgtjSLCg2zvgWbry3Hp3AuslrZPUA2wGtlUuIOkc4C+ByyPiYI61mOWifOZRqVoH2CwNuQVBRIwCVwP3AHuAuyJit6QbJF2eLfZp4Fjgi5IelLStxtuZzUovjIxyzII8O9Zm+ct1C46I7cD2CW3XVdy/OM/PN8vbwItFTlg8v91lmDXFA5tmTRgYLLB0cU+7yzBrioPArAlHhoosXeQegaXNQWDWhIHBAie4R2CJcxCYTVNEMDBYZKnnCCxxDgKzaRoujlMYHfccgSXPQWA2TQODBQDvNWTJcxCYTVM5CDw0ZKlzEJhN05HBIoCHhix5DgKzaRrIgsB7DVnqHARm0+ShIZsrHARm03RkqDw05CCwtDkIzKZp4MUCi3u6WNDtaxFY2hwEZtM0MOjTS9jc4CAwm6YjQz7hnM0NDgKzaRoYLHLCMe4RWPocBGbTdHiwwNJF7hFY+hwEZtN02CecsznCQWA2DRHB4SEHgc0NDgKzaTg6MsrYePioYpsTHARm0/CzgSEATj5uYZsrMWueg8BsGvYeOArAGSuObXMlZs1zEJhNwxMHXqBrnli37Jh2l2LWNAeB2TTsPXCUtSct9uklbE5wEJhNwxMHX+CMFUvaXYZZSzgIzKZouDjGT559kfUne37A5gYHgdkU7Tv0IuMB690jsDnCQWA2RU8cLO8x5CCwucFBYDZFew8cpdt7DNkc4iAwm4KI4Ns/OsSG1xxHT7d/fWxu8JZsNgU/+PFz7Hnmef7deWvaXYpZyzgIzKbglu89xdLF89n0xpXtLsWsZXINAkkbJT0uqU/SNVWeXyDpb7Pnvy9pbZ71mDXjO3sPcc/un7P5TWtY1OMDyWzu6M7rjSV1ATcClwD9wE5J2yLisYrFrgIGIuJ1kjYDnwTem1dNZlNVHBvnqV+8yNcefobP3dvHGSuW8MEL1rW7LLOWyi0IgPOAvojYByDpTmATUBkEm4Drs/tfAv6PJEVEtLqYu3Y+zdbv7ntFW62PqdpapbHactXes9Y/ptrHR5Wlqy43hTXUaE213rPhmqbwntWWbvw9G//eqq+7Br+jgBcLo4xnT1581gr+9L1vYMlCX4PA5pY8g2Al8HTF437gX9daJiJGJR0BTgJ+UbmQpC3AFoA1a6Y3SXfCMT2cWW2/b1Vfvlqz9OrW6ss1/DENv2e1RtV410Y/v/pyU3jPBgutvlxzNU3tPWut/frvuWRBN6tOXMwF65dx6vGLGnofs9TkGQTVfvsm/uHVyDJExFZgK0Bvb++0eguXbFjBJRtWTOelZmZzWp6Txf3A6orHq4D9tZaR1A0cDzyXY01mZjZBnkGwE1gvaZ2kHmAzsG3CMtuA92f33w18O4/5ATMzqy23oaFszP9q4B6gC7g5InZLugHYFRHbgM8Dt0nqo9QT2JxXPWZmVl2ecwRExHZg+4S26yruDwPvybMGMzObnI8sNjPrcA4CM7MO5yAwM+twDgIzsw6n1PbWlHQI+Mk0X76MCUctzxKua2pc19TN1tpc19Q0U9dpEbG82hPJBUEzJO2KiN521zGR65oa1zV1s7U21zU1edXloSEzsw7nIDAz63CdFgRb211ADa5ralzX1M3W2lzX1ORSV0fNEZiZ2at1Wo/AzMwmcBCYmXW4ORsEkt4jabekcUm9E577uKQ+SY9L+rWK9o1ZW5+ka2agxr+V9GB2e0rSg1n7WklDFc/dlHctE+q6XtLPKj7/sornqq67Garr05J+JOlhSV+RtDRrb+v6ymqY0W1nkjpWS7pX0p5s+/9w1l7zO53B2p6S9Ej2+buythMl/T9JT2Q/T5jhms6sWCcPSnpe0u+3Y31JulnSQUmPVrRVXT8q+Wy2vT0s6dymPjwi5uQNOAs4E7gP6K1o3wA8BCwA1gFPUjpNdld2/3SgJ1tmwwzW+xnguuz+WuDRNq6764H/UqW96rqbwbp+FejO7n8S+OQsWV9t3XYm1HIqcG52fwmwN/veqn6nM1zbU8CyCW2fAq7J7l9T/k7b+D3+HDitHesLuBA4t3JbrrV+gMuAuyld5fHNwPeb+ew52yOIiD0R8XiVpzYBd0bESET8GOgDzstufRGxLyIKwJ3ZsrlT6aK6vwncMROf14Ra625GRMQ3ImI0e7iD0lXvZoO2bTsTRcQzEfHD7P5RYA+la4PPVpuAW7P7twL/to21XAQ8GRHTPXNBUyLiO7z6Co211s8m4AtRsgNYKunU6X72nA2CSawEnq543J+11WqfCRcAByLiiYq2dZIekPSPki6YoToqXZ11OW+u6K63cx1N9DuU/iIqa+f6mk3r5SWS1gLnAN/Pmqp9pzMpgG9Iul/SlqxtRUQ8A6UQA05uQ11lm3nlH2PtXl9Qe/20dJtLOggkfVPSo1Vuk/01piptMUn7TNR4Ja/cAJ8B1kTEOcBHgNslHddsLVOo6y+A1wJvzGr5TPllVd6qpfsfN7K+JF0LjAJ/kzXlvr7qlV2lra37ZUs6Fvgy8PsR8Ty1v9OZ9JaIOBe4FPiQpAvbUENVKl1O93Lgi1nTbFhfk2npNpfrFcryFhEXT+Nl/cDqisergP3Z/Vrt01avRkndwG8Av1LxmhFgJLt/v6QngTOAXc3W02hdFfX9FfD17OFk625G6pL0fuCdwEWRDZbOxPqqI/f1MhWS5lMKgb+JiL8DiIgDFc9XfqczJiL2Zz8PSvoKpSG1A5JOjYhnsqGNgzNdV+ZS4Ifl9TQb1lem1vpp6TaXdI9gmrYBmyUtkLQOWA/8ANgJrJe0LvvrYHO2bN4uBn4UEf3lBknLJXVl90/Patw3A7WUP79yrPEKoLwXQ611N1N1bQQ+BlweEYMV7W1dX7Rv23mVbL7p88CeiPiTivZa3+lM1XWMpCXl+5Qm/h+ltJ7eny32fuDvZ7KuCq/olbd7fVWotX62Ae/L9h56M3CkPIQ0Le2YnZ+hGfgrKKXmCHAAuKfiuWsp7eXxOHBpRftllPayeBK4dobqvAX4jxPa3gXsprT3yQ+BX5/hdXcb8AjwcLbBnVpv3c1QXX2UxkUfzG43zYb11a5tp0Ydb6U0RPBwxXq6bLLvdIbqOj37fh7Kvqtrs/aTgG8BT2Q/T2zDOlsMPAscX9E24+uLUhA9AxSz/7uuqrV+KA0N3Zhtb49QsWfkdG4+xYSZWYfrxKEhMzOr4CAwM+twDgIzsw7nIDAz63AOAjOzDucgMDPrcA4CM7MO5yAwa5KkN2UnJ1uYHUG7W9LZ7a7LrFE+oMysBST9L2AhsAjoj4hPtLkks4Y5CMxaIDvH0E5gGDg/IsbaXJJZwzw0ZNYaJwLHUroq2MI212I2Je4RmLWApG2Urky2jtIJyq5uc0lmDUv6egRms4Gk9wGjEXF7djrs70l6R0R8u921mTXCPQIzsw7nOQIzsw7nIDAz63AOAjOzDucgMDPrcA4CM7MO5yAwM+twDgIzsw73/wE6PMd4vFS7swAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib, numpy and math \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math \n",
    "  \n",
    "x = np.linspace(-100, 100, 200) \n",
    "z = 1/(1 + np.exp(-x)) \n",
    "  \n",
    "plt.plot(x, z) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\") \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " #training data consisting of 4 examples--3 input values and 1 output\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math \n",
    "\n",
    "training_inputs = np.array([[0,0,1],\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]])\n",
    "training_outputs = np.array([[0,1,1,0]]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math \n",
    "    \n",
    "def sigmoid(self, x):\n",
    "    #applying the sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))   \n",
    "\n",
    "def sigmoid_derivative(self, x):\n",
    "    #computing derivative to the Sigmoid function\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        #training the model to make accurate predictions while adjusting weights continually\n",
    "        for iteration in range(training_iterations):\n",
    "            #siphon the training data via  the neuron\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            #computing error rate for back-propagation\n",
    "            error = training_outputs - output\n",
    "            \n",
    "            #performing weight adjustments\n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
    "\n",
    "            self.synaptic_weights += adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def think(self, inputs):\n",
    "        #passing the inputs via the neuron to get output   \n",
    "        #converting values to floats\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\luffy\\anaconda3\\lib\\site-packages (1.16.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For install PyTorch you need to install numpy first \n",
    "then run pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio===0.7.0 -f\n",
    "https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7010, 0.2518, 0.6508],\n",
      "        [0.7217, 0.0239, 0.3809],\n",
      "        [0.5052, 0.1482, 0.0845],\n",
      "        [0.2821, 0.4129, 0.7255],\n",
      "        [0.6480, 0.9840, 0.6339]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for check torch version then run pip install torchvision all cmmand in anaconda powershell prompt\n",
    "\n",
    "MNIST data setupÂ¶ We will use the classic MNIST http://deeplearning.net/data/mnist/_ dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
    "\n",
    "We will use pathlib https://docs.python.org/3/library/pathlib.html_ for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests http://docs.python-requests.org/en/master/_. We will only import modules when we use them, so you can see exactly what's being used at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net from scratch (no torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1243, -2.5024, -2.3716, -2.8423, -2.5475, -2.7290, -1.7224, -2.4946,\n",
      "        -2.4038, -1.8847], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our loss with our random model, so we can see if we improve\n",
    "after a backprop pass later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3444, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1250)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0811, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0811, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1761, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0819, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3841, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0818, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3078, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0808, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0828, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0820, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3121)\n",
      "1 tensor(0.2743)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fit() and get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4084534687280655\n",
      "1 0.3581363875865936\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.45028170275688173\n",
      "1 0.2089309791982174\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.387509791302681\n",
      "1 0.2669949719667435\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4373012679100037\n",
      "1 0.23867026481628417\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, we can move our model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.20518991851210594\n",
      "1 0.18799441893100738\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
